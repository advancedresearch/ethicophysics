\begin{verbatim}
tom 21:24
what’s the gist?
epurdy 21:24
um
21:24
i guess i need to write an abstract
21:25
but basically you can write down laws of ethics that are modeled on
 the laws of physics
tom 21:36
maybe you get to this later but why are love and hate defined in terms
of subsets?
21:36
rather than individuals
epurdy 21:36
well
21:36
you can hate british people, for instance
tom 21:36
so you can hate british people but you might not hate a specific few
of them?
epurdy 21:36
i should put in some sort of example like that
21:37
well you might not even know any british people
21:37
like i hate nazis, but i can't really name all of them
tom 21:37
but i mean if you defined love and hate in terms of individuals, one
might hate all british people individually
21:37
so it wouldn’t preclude that
epurdy 21:37
right....
21:37
hm
21:38
i think my definition is still better because it respects the
cognitive limitations
21:38
of the human mind
21:38
i can hate british people, but i am not capable of hating them all
individually
tom 21:38
hm interesting
epurdy 21:38
a computer could probably hate them all individually though
tom 21:39
and so do you think it is possible to have that you hate a set of
people a certain amount but you e.g. don’t hate one or two of them?
epurdy 21:39
hm
tom 21:39
i could see that making sense
epurdy 21:39
there are consistency issues i guess
21:39
if you hate british people with a passion but love your british wife
21:39
where does that leave one
tom 21:40
yeah
21:40
well certainly there are people in situations like that
epurdy 21:40
right but then the subset they hate is not really ``british people''
21:40
it's ``british people in general, not including my wife''
21:40
which is interesting
tom 21:40
yeah that’s true
tom 21:46
so what’s the connection to AI safety?
epurdy 21:46
\begin{abstract}
  What are Good and Evil? How do we explain these concepts to a computer
  sufficiently well that we can be assured that the computer will
  understand them in the same sense as humans understand them? These are
  hard questions, and people have often despaired of finding any answers
  to the AI safety problem.

  In this paper, we lay out a theory of ethics modeled on the laws of
  physics. The theory has two key advantages: it squares nicely with
  most human moral intuitions, and it is amenable to rather
  straightforward computations that a computer could easily perform if
  told to. It therefore forms an ideal foundation for solving the AI
  safety problem.
\end{abstract}
tom 21:47
gotcha
21:47
cool
epurdy 21:47
it's pretty good, right?
21:47
like i need to check the proofs a hundred more times but there's
something there i think
tom 21:48
i’m still digesting
21:48
could you give an example of how this would actually turn into
straightforward computations for AI safety?
epurdy 21:49
the AI could be given some mission in life
21:49
for instance, a babysitting AI could be given the mission to protect
the relevant child at all costs, without breaking the law
21:49
or something
21:50
the point is that you can escape the evil genie problem
21:50
because you can reason about multiple goals and how the tradeoffs
between them work
tom 21:50
what’s the evil genie problem?
epurdy 21:50
well, if you say protect the relevant child at all costs
21:51
and someone online says something mean to the child
21:51
a naively programmed AI would find and execute that person
tom 21:51
but if you programmed the AI so that it wasn’t allowed to break the
law, wouldn’t that be avoided in theory?
epurdy 21:52
but what if the law is fuzzy
21:52
or what if a law must be broken?
21:52
like you are running from a bad guy and you need to jaywalk in order
to escape
tom 21:52
hmm i see, and how would ethicophysics deal with that scenario?
epurdy 21:53
it would make an easy snap judgment that no one is going to give a
shit about jaywalking if the kid is going to die
21:53
you can reason through shit like that over time, but ideally the AI
would know the answer immediately without thinking about it
tom 21:53
how exactly though?
epurdy 21:54
so it is trying to maximize l(God, {robot}) - h(God, {robot}), say
21:55
and then it is doing planning in the RL setting, which we know will
work when AI gets to that point
21:55
but it has a preposterous number of heuristics that it can apply
21:55
and the heuristics are actually provable laws of ethics derived from
ethicophysical proofs that are mathematically tight
tom 21:56
but how would it calculate  l(God, {robot}) - h(God, {robot})?
epurdy 21:56
well there it needs some sort of world model that includes the
ethicophysics components
21:57
so we don't know yet exactly what htat would look like
21:57
but writing down a bunch of ethical laws that can be used in
simulations can only help make matters better
tom 21:58
isn’t the hardest part of the AI safety problem figuring out how to
precisely describe what the AI should be optimizing though?
epurdy 21:58
yes!
21:58
which is why this is so exciting
21:58
it's the beginnings of a precise description of that
tom 21:59
but when i say
>precisely describe what the AI should be optimizing
what i mean is something like a complete definition of l(God, {robot})
and h(God, {robot}). like isn’t that just a rephrasing of the
question basically
epurdy 21:59
right, but now we have intermediate concepts that we are defining
21:59
like coherent extrapolated ethical momentum
22:00
these concepts allow us to build a common vocabulary with the AI
22:00
so we don't have to assume that some deep network somewhere knows what
we want it to know
22:00
we can write automated theorem provers around the laws of
ethicophysics and then get scrutable ethical proofs of what the right
thing to do probably is
22:01
then we can test the shit out of those automated theorem provers
22:01
make sure they work using traditional software practices
22:01
then we should be good to go
tom 22:02
okay well a lot of that is still fuzzy for me but i have to jump off
for the night, thanks for sharing! looking forward to talking more
about it later
epurdy 22:02
cool
22:02
thanks for not calling me crazy
\end{verbatim}
